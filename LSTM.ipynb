{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4868000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20148863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"TA_Training_Set.csv\")\n",
    "df_test = pd.read_csv(\"TestFileTemplate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631f01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.sample(n=100000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6639f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mean the white walkers, not the wights. Was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If that's your standard for spoilers  I hope y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That was a big reason why a lot used the Glitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middleton probably does have green sweat consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow someone posted detected repost and linked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Did we forget that hes a flaming pile of shit?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I can't, not again. I'm not strong enough.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wooow you are so pretty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TIL I am a very passionate person.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I and a few other guys had a bottle of whiskey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Is this like a discount pun patrol? Just a bun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>how would Josuke be pestilence? he'd fix the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Not to mention his wallet and phone are probab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Was trying to figure that out myself. Looked/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hunted them in a wooded area behind a house in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I'm with you here. Mueller got gun shy and now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Of course you say ignore the refs Shaq. You al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I don't think she has any understandable conce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gaming is my hobby and should be viewed as suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Rooting for them, they took a huge gamble at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>literally all over all of my sweatshirts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Corey is overestimating how hard a steel chair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>you alright there bud?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I see nothing wrong with anything you said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I'm not saying that the battle or the plan was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What happens with the Snap though? Does the Sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>he lost that money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Those are all the ghost thoughts of the guys t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Coors light is ideal here. We call this drink ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Of course, Buffalo Alistar skin incoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Please take this person's advise!!!! I have se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>There's a reason why these games are often ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>You're entitled to your opinion but  I'm also ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Yeah ditto, probably watch it again at some point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>\"Leftists want to spin everything to make Trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>brb going to apply riot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Thanks for the tips I've been getting my ass h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>You realize we are talking about civilian airc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10/10, would face plant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>So I tried something similar today. Had a path...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Well the neighbor kid was mentally handicapped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>I have the same problem my friend ;(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>I dont get it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Fixing the CGI is one thing, but the main issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Shit in your hat bud. Boo hoo the Internet is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Talk to me when i had dairy.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comment\n",
       "0   I mean the white walkers, not the wights. Was ...\n",
       "1   If that's your standard for spoilers  I hope y...\n",
       "2   That was a big reason why a lot used the Glitc...\n",
       "3   Middleton probably does have green sweat consi...\n",
       "4   Wow someone posted detected repost and linked ...\n",
       "5   Did we forget that hes a flaming pile of shit?...\n",
       "7          I can't, not again. I'm not strong enough.\n",
       "8                             Wooow you are so pretty\n",
       "9                  TIL I am a very passionate person.\n",
       "10  I and a few other guys had a bottle of whiskey...\n",
       "11  Is this like a discount pun patrol? Just a bun...\n",
       "12  how would Josuke be pestilence? he'd fix the p...\n",
       "13  Not to mention his wallet and phone are probab...\n",
       "14  Was trying to figure that out myself. Looked/s...\n",
       "15  Hunted them in a wooded area behind a house in...\n",
       "16  I'm with you here. Mueller got gun shy and now...\n",
       "17  Of course you say ignore the refs Shaq. You al...\n",
       "18  I don't think she has any understandable conce...\n",
       "19  Gaming is my hobby and should be viewed as suc...\n",
       "21  Rooting for them, they took a huge gamble at t...\n",
       "22           literally all over all of my sweatshirts\n",
       "23  Corey is overestimating how hard a steel chair...\n",
       "24                             you alright there bud?\n",
       "25        I see nothing wrong with anything you said.\n",
       "26  I'm not saying that the battle or the plan was...\n",
       "28  What happens with the Snap though? Does the Sn...\n",
       "29                                 he lost that money\n",
       "30  Those are all the ghost thoughts of the guys t...\n",
       "31  Coors light is ideal here. We call this drink ...\n",
       "32           Of course, Buffalo Alistar skin incoming\n",
       "34  Please take this person's advise!!!! I have se...\n",
       "35  There's a reason why these games are often ref...\n",
       "36  You're entitled to your opinion but  I'm also ...\n",
       "37  Yeah ditto, probably watch it again at some point\n",
       "38  \"Leftists want to spin everything to make Trum...\n",
       "40                           brb going to apply riot.\n",
       "41  Thanks for the tips I've been getting my ass h...\n",
       "42  You realize we are talking about civilian airc...\n",
       "43                           10/10, would face plant.\n",
       "44  So I tried something similar today. Had a path...\n",
       "45  Well the neighbor kid was mentally handicapped...\n",
       "47               I have the same problem my friend ;(\n",
       "48                                     I dont get it?\n",
       "50  Fixing the CGI is one thing, but the main issu...\n",
       "51  Shit in your hat bud. Boo hoo the Internet is ...\n",
       "52                       Talk to me when i had dairy."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=df_test.loc[df_test['Comment'].str.contains(r'[^\\x00-\\x7F]+') == False]\n",
    "df_train=df_train.loc[df_train['Comment'].str.contains(r'[^\\x00-\\x7F]+') == False]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54d4cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df_train['x_without_stopwords'] = df_train['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "\n",
    "df_test['x_without_stopwords'] = df_test['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a05c3c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-c1b26f9c7b70>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_train[\"x_without_stopwords\"] = df_train[\"x_without_stopwords\"].str.replace('\\d+', '')\n",
      "<ipython-input-6-c1b26f9c7b70>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_test[\"x_without_stopwords\"] = df_test[\"x_without_stopwords\"].str.replace('\\d+', '')\n",
      "<ipython-input-6-c1b26f9c7b70>:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df_train[\"x_without_stopwords\"] = df_train[\"x_without_stopwords\"].str.replace(i,'')\n",
      "<ipython-input-6-c1b26f9c7b70>:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df_test[\"x_without_stopwords\"] = df_test[\"x_without_stopwords\"].str.replace(i,'')\n"
     ]
    }
   ],
   "source": [
    "symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\nÄô\"\n",
    "for i in symbols:\n",
    "    df_train[\"x_without_stopwords\"] = df_train[\"x_without_stopwords\"].str.replace(i,'')\n",
    "    df_train[\"x_without_stopwords\"] = df_train[\"x_without_stopwords\"].str.replace('\\d+', '')\n",
    "    df_test[\"x_without_stopwords\"] = df_test[\"x_without_stopwords\"].str.replace(i,'')\n",
    "    df_test[\"x_without_stopwords\"] = df_test[\"x_without_stopwords\"].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f92729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Topic</th>\n",
       "      <th>x_without_stopwords</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527984</th>\n",
       "      <td>I'm destroying kids online with kitana. Underr...</td>\n",
       "      <td>7</td>\n",
       "      <td>I'm destroying kids online kitana Underrated imo</td>\n",
       "      <td>[I, 'm, destroying, kids, online, kitana, Unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618093</th>\n",
       "      <td>My guess : Not Riot job to work on this issue ...</td>\n",
       "      <td>23</td>\n",
       "      <td>My guess  Not Riot job work issue Russia proba...</td>\n",
       "      <td>[My, guess, Not, Riot, job, work, issue, Russi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484217</th>\n",
       "      <td>Hemsworth bending the knee in the early photo ...</td>\n",
       "      <td>24</td>\n",
       "      <td>Hemsworth bending knee early photo either 'cau...</td>\n",
       "      <td>[Hemsworth, bending, knee, early, photo, eithe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161123</th>\n",
       "      <td>Oh that's more appropriate...since I did eats ...</td>\n",
       "      <td>30</td>\n",
       "      <td>Oh that's appropriatesince I eats</td>\n",
       "      <td>[Oh, that, 's, appropriatesince, I, eats]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234835</th>\n",
       "      <td>The fact that there are multiple points of act...</td>\n",
       "      <td>26</td>\n",
       "      <td>The fact multiple points action flow one anoth...</td>\n",
       "      <td>[The, fact, multiple, points, action, flow, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Comment  Topic  \\\n",
       "527984  I'm destroying kids online with kitana. Underr...      7   \n",
       "618093  My guess : Not Riot job to work on this issue ...     23   \n",
       "484217  Hemsworth bending the knee in the early photo ...     24   \n",
       "161123  Oh that's more appropriate...since I did eats ...     30   \n",
       "234835  The fact that there are multiple points of act...     26   \n",
       "\n",
       "                                      x_without_stopwords  \\\n",
       "527984   I'm destroying kids online kitana Underrated imo   \n",
       "618093  My guess  Not Riot job work issue Russia proba...   \n",
       "484217  Hemsworth bending knee early photo either 'cau...   \n",
       "161123                 Oh that's appropriatesince I eats    \n",
       "234835  The fact multiple points action flow one anoth...   \n",
       "\n",
       "                                                tokenized  \n",
       "527984  [I, 'm, destroying, kids, online, kitana, Unde...  \n",
       "618093  [My, guess, Not, Riot, job, work, issue, Russi...  \n",
       "484217  [Hemsworth, bending, knee, early, photo, eithe...  \n",
       "161123          [Oh, that, 's, appropriatesince, I, eats]  \n",
       "234835  [The, fact, multiple, points, action, flow, on...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "df_train['x_without_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "df_train[\"tokenized\"]=df_train['x_without_stopwords'].apply(word_tokenize)\n",
    "\n",
    "\n",
    "df_test['x_without_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "df_test[\"tokenized\"]=df_test['x_without_stopwords'].apply(word_tokenize)\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d8f858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 23, 24, 30, 26, 25,  1, 35, 16, 31, 36,  2,  5, 27, 11,  9, 20,\n",
       "       38, 18, 34,  4, 33, 19, 17, 12, 21,  3, 15, 10, 13, 40, 22, 14, 32,\n",
       "       39,  6, 28, 29, 37,  8], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfdbfa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#from sklearn.datasets.samples_generator import make_blobs\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff45e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:57502\n",
      "Class distributionCounter({20: 1612, 30: 1584, 29: 1576, 16: 1568, 25: 1568, 18: 1560, 33: 1559, 35: 1553, 2: 1552, 23: 1547, 21: 1541, 4: 1536, 10: 1535, 14: 1528, 12: 1517, 26: 1499, 11: 1498, 38: 1498, 40: 1494, 28: 1464, 39: 1457, 31: 1456, 22: 1449, 7: 1448, 3: 1446, 27: 1443, 9: 1441, 15: 1438, 37: 1430, 17: 1425, 19: 1419, 36: 1401, 5: 1398, 24: 1387, 13: 1386, 34: 1364, 1: 1319, 32: 1296, 6: 1276, 8: 34})\n",
      "Valid data len:24645\n",
      "Class distributionCounter({23: 753, 26: 702, 16: 687, 12: 680, 22: 678, 9: 671, 30: 668, 4: 668, 20: 663, 33: 658, 18: 647, 39: 646, 21: 646, 35: 645, 2: 645, 40: 639, 15: 638, 14: 637, 25: 637, 28: 634, 34: 632, 19: 628, 5: 626, 13: 623, 27: 623, 17: 622, 10: 620, 38: 620, 31: 615, 29: 609, 7: 602, 36: 601, 3: 593, 24: 589, 11: 586, 1: 572, 32: 560, 37: 549, 6: 522, 8: 11})\n"
     ]
    }
   ],
   "source": [
    "X= df_train['tokenized'].apply(str)\n",
    "y =df_train['Topic']\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.3,random_state=8)\n",
    "\n",
    "\n",
    "\n",
    "print('Train data len:'+str(len(X_train)))\n",
    "print('Class distribution'+str(Counter(y_train)))\n",
    "print('Valid data len:'+str(len(X_valid)))\n",
    "print('Class distribution'+ str(Counter(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e032e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018f3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-a62070985974>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
      "<ipython-input-12-a62070985974>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
      "<ipython-input-12-a62070985974>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test  = np.array( tokenizer.texts_to_sequences(df_train[\"x_without_stopwords\"].tolist()) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len:57502\n",
      "Class distributionCounter({20: 1612, 30: 1584, 29: 1576, 16: 1568, 25: 1568, 18: 1560, 33: 1559, 35: 1553, 2: 1552, 23: 1547, 21: 1541, 4: 1536, 10: 1535, 14: 1528, 12: 1517, 26: 1499, 11: 1498, 38: 1498, 40: 1494, 28: 1464, 39: 1457, 31: 1456, 22: 1449, 7: 1448, 3: 1446, 27: 1443, 9: 1441, 15: 1438, 37: 1430, 17: 1425, 19: 1419, 36: 1401, 5: 1398, 24: 1387, 13: 1386, 34: 1364, 1: 1319, 32: 1296, 6: 1276, 8: 34})\n",
      "Validation data len:24645\n",
      "Class distributionCounter({23: 753, 26: 702, 16: 687, 12: 680, 22: 678, 9: 671, 30: 668, 4: 668, 20: 663, 33: 658, 18: 647, 39: 646, 21: 646, 35: 645, 2: 645, 40: 639, 15: 638, 14: 637, 25: 637, 28: 634, 34: 632, 19: 628, 5: 626, 13: 623, 27: 623, 17: 622, 10: 620, 38: 620, 31: 615, 29: 609, 7: 602, 36: 601, 3: 593, 24: 589, 11: 586, 1: 572, 32: 560, 37: 549, 6: 522, 8: 11})\n",
      "Test data len:82147\n",
      "Class distributionCounter({23: 2300, 20: 2275, 16: 2255, 30: 2252, 33: 2217, 18: 2207, 25: 2205, 4: 2204, 26: 2201, 35: 2198, 2: 2197, 12: 2197, 21: 2187, 29: 2185, 14: 2165, 10: 2155, 40: 2133, 22: 2127, 38: 2118, 9: 2112, 39: 2103, 28: 2098, 11: 2084, 15: 2076, 31: 2071, 27: 2066, 7: 2050, 19: 2047, 17: 2047, 3: 2039, 5: 2024, 13: 2009, 36: 2002, 34: 1996, 37: 1979, 24: 1976, 1: 1891, 32: 1856, 6: 1798, 8: 45})\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array( tokenizer.texts_to_sequences(X_train) )\n",
    "x_valid = np.array( tokenizer.texts_to_sequences(X_valid) )\n",
    "x_test  = np.array( tokenizer.texts_to_sequences(df_train[\"x_without_stopwords\"].tolist()) )\n",
    "\n",
    "\n",
    "\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=100)\n",
    "x_valid = pad_sequences(x_valid, padding='post', maxlen=100)\n",
    "x_test = pad_sequences(x_test, padding='post', maxlen=100)\n",
    "\n",
    "\n",
    "\n",
    "train_labels = np.asarray(y_train )\n",
    "valid_labels = np.asarray( y_valid)\n",
    "\n",
    "test_labels = np.asarray(df_train[\"Topic\"].tolist())\n",
    "\n",
    "print('Train data len:'+str(len(x_train)))\n",
    "print('Class distribution'+str(Counter(train_labels)))\n",
    "\n",
    "print('Validation data len:'+str(len(x_valid)))\n",
    "print('Class distribution'+str(Counter(valid_labels)))\n",
    "\n",
    "print('Test data len:'+str(len(x_test)))\n",
    "print('Class distribution'+str(Counter(test_labels)))\n",
    "\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train,train_labels))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid,valid_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test,test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6922b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           800016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 16)           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 16)           2112      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               819712    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 4104      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,953\n",
      "Trainable params: 1,625,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features =50000\n",
    "embedding_dim =16\n",
    "sequence_length = 100\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\n",
    "                                    embeddings_regularizer = regularizers.l2(0.005))) \n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(embedding_dim,dropout=0.2, recurrent_dropout=0.2,return_sequences=True,\\\n",
    "                                                             kernel_regularizer=regularizers.l2(0.005),\\\n",
    "                                                             bias_regularizer=regularizers.l2(0.005)))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(8, activation='relu',\\\n",
    "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
    "                                bias_regularizer=regularizers.l2(0.001),))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(1e-3),metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "697785c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "57/57 [==============================] - 15s 231ms/step - loss: -11410.8896 - binary_accuracy: 0.0229 - val_loss: -60174.4961 - val_binary_accuracy: 0.0232\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 15s 273ms/step - loss: -239565.3750 - binary_accuracy: 0.0229 - val_loss: -545309.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 16s 279ms/step - loss: -1069780.7500 - binary_accuracy: 0.0229 - val_loss: -1798874.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 17s 307ms/step - loss: -2830746.2500 - binary_accuracy: 0.0229 - val_loss: -4135700.7500 - val_binary_accuracy: 0.0232\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 17s 304ms/step - loss: -5811840.0000 - binary_accuracy: 0.0229 - val_loss: -7877214.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 17s 296ms/step - loss: -10392667.0000 - binary_accuracy: 0.0229 - val_loss: -13346843.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 17s 294ms/step - loss: -16914824.0000 - binary_accuracy: 0.0229 - val_loss: -20884316.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 16s 288ms/step - loss: -25507718.0000 - binary_accuracy: 0.0229 - val_loss: -30739522.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 17s 299ms/step - loss: -36477932.0000 - binary_accuracy: 0.0229 - val_loss: -43170720.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 17s 297ms/step - loss: -50417896.0000 - binary_accuracy: 0.0229 - val_loss: -58548988.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 17s 301ms/step - loss: -67493656.0000 - binary_accuracy: 0.0229 - val_loss: -77105384.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 17s 298ms/step - loss: -87343624.0000 - binary_accuracy: 0.0229 - val_loss: -98912064.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 16s 288ms/step - loss: -110853032.0000 - binary_accuracy: 0.0229 - val_loss: -124318512.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 16s 285ms/step - loss: -138248464.0000 - binary_accuracy: 0.0229 - val_loss: -153555280.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 16s 286ms/step - loss: -168773312.0000 - binary_accuracy: 0.0229 - val_loss: -186653968.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 16s 279ms/step - loss: -203724448.0000 - binary_accuracy: 0.0229 - val_loss: -223890976.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 16s 285ms/step - loss: -244386880.0000 - binary_accuracy: 0.0229 - val_loss: -265647424.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 16s 289ms/step - loss: -287785440.0000 - binary_accuracy: 0.0229 - val_loss: -311879104.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 17s 295ms/step - loss: -336336448.0000 - binary_accuracy: 0.0229 - val_loss: -362634304.0000 - val_binary_accuracy: 0.0232\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 16s 289ms/step - loss: -388168704.0000 - binary_accuracy: 0.0229 - val_loss: -418115136.0000 - val_binary_accuracy: 0.0232\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Fit the model using the train and test datasets.\n",
    "#history = model.fit(x_train, train_labels,validation_data= (x_test,test_labels),epochs=epochs )\n",
    "history = model.fit(train_ds.shuffle(5000).batch(1024),\n",
    "                    epochs= epochs ,\n",
    "                    validation_data=valid_ds.batch(1024),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf0160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
